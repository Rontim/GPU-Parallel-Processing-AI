{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eL-O_WVEq6mz"
      },
      "source": [
        "# MNIST GAN from Scratch (with GPU Acceleration)\n",
        "## Objective:\n",
        "To demonstrate the power of GPU-accelerated parallel processing using a Generative Adversarial Network (GAN) trained on the MNIST dataset. We'll generate synthetic images of handwritten digits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp9AVhDRq6m1"
      },
      "source": [
        "## Setup\n",
        "We will use TensorFlow which automatically utilizes the GPU if any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "34s0GmXRq6m2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LeakyReLU, BatchNormalization, Reshape, Flatten, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "lvQBM8nzq6m3"
      },
      "outputs": [],
      "source": [
        "# Create output directory for images\n",
        "if not os.path.exists(\"gan_images\"):\n",
        "    os.makedirs(\"gan_images\")\n",
        "\n",
        "# Load and preprocess the data\n",
        "def load_data():\n",
        "    (X_train, _), (_, _) = mnist.load_data()\n",
        "    # Normalize and reshape\n",
        "    X_train = X_train / 127.5 - 1.0\n",
        "    X_train = np.expand_dims(X_train, axis=3)\n",
        "    return X_train\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator Model"
      ],
      "metadata": {
        "id": "CYNw97GFryXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(latent_dim=100):\n",
        "    model = Sequential([\n",
        "        # First layer\n",
        "        Dense(256, input_dim=latent_dim),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "\n",
        "        # Second layer\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "\n",
        "        # Third layer\n",
        "        Dense(1024),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        BatchNormalization(momentum=0.8),\n",
        "\n",
        "        # Output layer\n",
        "        Dense(28*28*1, activation='tanh'),\n",
        "        Reshape((28, 28, 1))\n",
        "    ])\n",
        "\n",
        "    noise = Input(shape=(latent_dim,))\n",
        "    img = model(noise)\n",
        "\n",
        "    return Model(noise, img)"
      ],
      "metadata": {
        "id": "l-wW66i0r2qb"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discriminator Model"
      ],
      "metadata": {
        "id": "sL7hFn94sECt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the discriminator network\n",
        "def build_discriminator(img_shape=(28, 28, 1)):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=img_shape),\n",
        "        Dense(512),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Dense(256),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    img = Input(shape=img_shape)\n",
        "    validity = model(img)\n",
        "\n",
        "    return Model(img, validity)"
      ],
      "metadata": {
        "id": "CxxAEsvhsHUC"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train Loop"
      ],
      "metadata": {
        "id": "Shey2umcubqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to sample images and visualize them\n",
        "def sample_images(generator, epoch, latent_dim=100):\n",
        "    r, c = 5, 5\n",
        "    noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
        "    gen_imgs = generator.predict(noise, verbose=0)\n",
        "\n",
        "    # Rescale images from [-1, 1] to [0, 1]\n",
        "    gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "    fig, axs = plt.subplots(r, c, figsize=(15, 15))\n",
        "    cnt = 0\n",
        "    for i in range(r):\n",
        "        for j in range(c):\n",
        "            axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
        "            axs[i, j].axis('off')\n",
        "            cnt += 1\n",
        "\n",
        "    fig.savefig(f\"gan_images/mnist_{epoch}.png\")\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "rjU5_GoBum_a"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Improved GAN training class using custom training loop\n",
        "class MNIST_GAN:\n",
        "    def __init__(self, latent_dim=100):\n",
        "        self.img_shape = (28, 28, 1)\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        # Build and compile discriminator\n",
        "        self.discriminator = build_discriminator(self.img_shape)\n",
        "        self.discriminator.compile(\n",
        "            optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Build generator\n",
        "        self.generator = build_generator(self.latent_dim)\n",
        "\n",
        "        # For the combined model, we only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Build the combined model\n",
        "        z = Input(shape=(self.latent_dim,))\n",
        "        img = self.generator(z)\n",
        "        valid = self.discriminator(img)\n",
        "        self.combined = Model(z, valid)\n",
        "        self.combined.compile(\n",
        "            optimizer=Adam(learning_rate=0.0002, beta_1=0.5),\n",
        "            loss='binary_crossentropy'\n",
        "        )\n",
        "\n",
        "        # Initialize history\n",
        "        self.history = {\n",
        "            'd_loss': [], 'd_acc': [], 'g_loss': []\n",
        "        }\n",
        "\n",
        "    def train(self, X_train, epochs, batch_size, sample_interval=1000, save_interval=5000):\n",
        "        # Label smoothing values\n",
        "        real_label_value = 0.9\n",
        "        fake_label_value = 0.1\n",
        "\n",
        "        # Adversarial ground truths\n",
        "        valid = np.ones((batch_size//2, 1))\n",
        "        fake = np.zeros((batch_size//2, 1))\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            # Select a random half of images\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size//2)\n",
        "            imgs = X_train[idx]\n",
        "\n",
        "            # Add noise to real images\n",
        "            imgs_noisy = imgs + np.random.normal(0, 0.1, imgs.shape)\n",
        "            imgs_noisy = np.clip(imgs_noisy, -1, 1)\n",
        "\n",
        "            # Sample noise and generate a half batch of new images\n",
        "            noise = np.random.normal(0, 1, (batch_size//2, self.latent_dim))\n",
        "            gen_imgs = self.generator.predict(noise, verbose=0)\n",
        "\n",
        "            # Apply label smoothing\n",
        "            valid_smooth = valid * real_label_value\n",
        "            fake_smooth = fake + fake_label_value\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch(imgs_noisy, valid_smooth)\n",
        "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake_smooth)\n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            # Generate a batch of noise\n",
        "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.combined.train_on_batch(noise, np.ones((batch_size, 1)) * real_label_value)\n",
        "\n",
        "            # Update history\n",
        "            self.history['d_loss'].append(d_loss[0])\n",
        "            self.history['d_acc'].append(d_loss[1])\n",
        "            self.history['g_loss'].append(g_loss)\n",
        "\n",
        "            # Print progress\n",
        "            print_interval = max(1, min(100, epochs // 20))\n",
        "            if epoch % print_interval == 0:\n",
        "                print(f\"Epoch {epoch}/{epochs} [D loss: {d_loss[0]:.4f}, acc: {100*d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]\")\n",
        "\n",
        "            # Save sample images\n",
        "            if epoch % sample_interval == 0:\n",
        "                sample_images(self.generator, epoch, self.latent_dim)\n",
        "\n",
        "            # Save models\n",
        "            if epoch % save_interval == 0 and epoch > 0:\n",
        "                self.save_models(epoch)\n",
        "\n",
        "            # Learning rate decay (using callbacks would be cleaner but this works)\n",
        "            if epoch % 1000 == 0 and epoch > 0:\n",
        "                # Manually decay learning rates\n",
        "                d_lr = self.discriminator.optimizer.learning_rate.numpy()\n",
        "                g_lr = self.combined.optimizer.learning_rate.numpy()\n",
        "\n",
        "                # Apply decay factor\n",
        "                self.discriminator.optimizer.learning_rate = d_lr * 0.95\n",
        "                self.combined.optimizer.learning_rate = g_lr * 0.95\n",
        "\n",
        "    def save_models(self, epoch):\n",
        "        self.generator.save(f\"generator_epoch_{epoch}.keras\")\n",
        "        self.discriminator.save(f\"discriminator_epoch_{epoch}.keras\")\n",
        "\n",
        "    def plot_history(self):\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # Plot discriminator metrics\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.history['d_loss'], label='Discriminator Loss')\n",
        "        plt.plot(self.history['d_acc'], label='Discriminator Accuracy')\n",
        "        plt.title('Discriminator Performance')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot generator loss\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.history['g_loss'], label='Generator Loss')\n",
        "        plt.title('Generator Performance')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('gan_training_history.png')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Pmf63yGtuTrb"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lift off>>>>>"
      ],
      "metadata": {
        "id": "XlcVL1T4uw5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Load data\n",
        "    X_train = load_data()\n",
        "\n",
        "    # Initialize GAN\n",
        "    gan = MNIST_GAN(latent_dim=100)\n",
        "\n",
        "    # Training parameters\n",
        "    epochs = 30000\n",
        "    batch_size = 32\n",
        "    sample_interval = 1000\n",
        "    save_interval = 5000\n",
        "\n",
        "    # Train the GAN\n",
        "    gan.train(\n",
        "        X_train=X_train,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        sample_interval=sample_interval,\n",
        "        save_interval=save_interval\n",
        "    )\n",
        "\n",
        "    # Plot training history\n",
        "    gan.plot_history()\n",
        "\n",
        "    # Generate final images\n",
        "    sample_images(gan.generator, \"final\")\n",
        "\n",
        "    # Save final models\n",
        "    gan.generator.save(\"generator_final.keras\")\n",
        "    gan.discriminator.save(\"discriminator_final.keras\")\n",
        "\n",
        "    print(\"GAN training complete!\")"
      ],
      "metadata": {
        "id": "9_Td7jvlu09D",
        "outputId": "eb63f4c2-ad19-4486-d97c-744e4fed6c37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/30000 [D loss: 0.5719, acc: 0.00%] [G loss: 0.6392]\n",
            "Epoch 100/30000 [D loss: 1.1082, acc: 0.00%] [G loss: 0.3740]\n",
            "Epoch 200/30000 [D loss: 1.2063, acc: 0.00%] [G loss: 0.3526]\n",
            "Epoch 300/30000 [D loss: 1.2388, acc: 0.00%] [G loss: 0.3451]\n",
            "Epoch 400/30000 [D loss: 1.2564, acc: 0.00%] [G loss: 0.3412]\n",
            "Epoch 500/30000 [D loss: 1.2657, acc: 0.00%] [G loss: 0.3389]\n",
            "Epoch 600/30000 [D loss: 1.2723, acc: 0.00%] [G loss: 0.3373]\n",
            "Epoch 700/30000 [D loss: 1.2774, acc: 0.00%] [G loss: 0.3361]\n",
            "Epoch 800/30000 [D loss: 1.2814, acc: 0.00%] [G loss: 0.3352]\n",
            "Epoch 900/30000 [D loss: 1.2841, acc: 0.00%] [G loss: 0.3345]\n",
            "Epoch 1000/30000 [D loss: 1.2866, acc: 0.00%] [G loss: 0.3339]\n",
            "Epoch 1100/30000 [D loss: 1.2885, acc: 0.00%] [G loss: 0.3334]\n",
            "Epoch 1200/30000 [D loss: 1.2904, acc: 0.00%] [G loss: 0.3330]\n",
            "Epoch 1300/30000 [D loss: 1.2917, acc: 0.00%] [G loss: 0.3326]\n",
            "Epoch 1400/30000 [D loss: 1.2929, acc: 0.00%] [G loss: 0.3323]\n",
            "Epoch 1500/30000 [D loss: 1.2938, acc: 0.00%] [G loss: 0.3320]\n",
            "Epoch 1600/30000 [D loss: 1.2944, acc: 0.00%] [G loss: 0.3318]\n",
            "Epoch 1700/30000 [D loss: 1.2951, acc: 0.00%] [G loss: 0.3316]\n",
            "Epoch 1800/30000 [D loss: 1.2958, acc: 0.00%] [G loss: 0.3314]\n",
            "Epoch 1900/30000 [D loss: 1.2964, acc: 0.00%] [G loss: 0.3312]\n",
            "Epoch 2000/30000 [D loss: 1.2965, acc: 0.00%] [G loss: 0.3311]\n",
            "Epoch 2100/30000 [D loss: 1.2968, acc: 0.00%] [G loss: 0.3309]\n",
            "Epoch 2200/30000 [D loss: 1.2971, acc: 0.00%] [G loss: 0.3308]\n",
            "Epoch 2300/30000 [D loss: 1.2975, acc: 0.00%] [G loss: 0.3307]\n",
            "Epoch 2400/30000 [D loss: 1.2978, acc: 0.00%] [G loss: 0.3305]\n",
            "Epoch 2500/30000 [D loss: 1.2982, acc: 0.00%] [G loss: 0.3304]\n",
            "Epoch 2600/30000 [D loss: 1.2984, acc: 0.00%] [G loss: 0.3303]\n",
            "Epoch 2700/30000 [D loss: 1.2985, acc: 0.00%] [G loss: 0.3302]\n",
            "Epoch 2800/30000 [D loss: 1.2987, acc: 0.00%] [G loss: 0.3301]\n",
            "Epoch 2900/30000 [D loss: 1.2989, acc: 0.00%] [G loss: 0.3300]\n",
            "Epoch 3000/30000 [D loss: 1.2990, acc: 0.00%] [G loss: 0.3299]\n",
            "Epoch 3100/30000 [D loss: 1.2992, acc: 0.00%] [G loss: 0.3298]\n",
            "Epoch 3200/30000 [D loss: 1.2994, acc: 0.00%] [G loss: 0.3297]\n",
            "Epoch 3300/30000 [D loss: 1.2994, acc: 0.00%] [G loss: 0.3297]\n",
            "Epoch 3400/30000 [D loss: 1.2996, acc: 0.00%] [G loss: 0.3296]\n",
            "Epoch 3500/30000 [D loss: 1.2998, acc: 0.00%] [G loss: 0.3295]\n",
            "Epoch 3600/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3294]\n",
            "Epoch 3700/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3293]\n",
            "Epoch 3800/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3293]\n",
            "Epoch 3900/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3292]\n",
            "Epoch 4000/30000 [D loss: 1.2998, acc: 0.00%] [G loss: 0.3291]\n",
            "Epoch 4100/30000 [D loss: 1.2998, acc: 0.00%] [G loss: 0.3291]\n",
            "Epoch 4200/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3290]\n",
            "Epoch 4300/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3289]\n",
            "Epoch 4400/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3289]\n",
            "Epoch 4500/30000 [D loss: 1.3000, acc: 0.00%] [G loss: 0.3288]\n",
            "Epoch 4600/30000 [D loss: 1.3000, acc: 0.00%] [G loss: 0.3287]\n",
            "Epoch 4700/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3287]\n",
            "Epoch 4800/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3286]\n",
            "Epoch 4900/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3285]\n",
            "Epoch 5000/30000 [D loss: 1.3002, acc: 0.00%] [G loss: 0.3285]\n",
            "Epoch 5100/30000 [D loss: 1.3002, acc: 0.00%] [G loss: 0.3284]\n",
            "Epoch 5200/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3284]\n",
            "Epoch 5300/30000 [D loss: 1.3002, acc: 0.00%] [G loss: 0.3283]\n",
            "Epoch 5400/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3283]\n",
            "Epoch 5500/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3282]\n",
            "Epoch 5600/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3282]\n",
            "Epoch 5700/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3281]\n",
            "Epoch 5800/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3281]\n",
            "Epoch 5900/30000 [D loss: 1.3001, acc: 0.00%] [G loss: 0.3280]\n",
            "Epoch 6000/30000 [D loss: 1.3000, acc: 0.00%] [G loss: 0.3280]\n",
            "Epoch 6100/30000 [D loss: 1.3000, acc: 0.00%] [G loss: 0.3279]\n",
            "Epoch 6200/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3279]\n",
            "Epoch 6300/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3279]\n",
            "Epoch 6400/30000 [D loss: 1.2999, acc: 0.00%] [G loss: 0.3278]\n",
            "Epoch 6500/30000 [D loss: 1.2998, acc: 0.00%] [G loss: 0.3278]\n",
            "Epoch 6600/30000 [D loss: 1.2998, acc: 0.00%] [G loss: 0.3277]\n",
            "Epoch 6700/30000 [D loss: 1.2997, acc: 0.00%] [G loss: 0.3277]\n",
            "Epoch 6800/30000 [D loss: 1.2997, acc: 0.00%] [G loss: 0.3277]\n",
            "Epoch 6900/30000 [D loss: 1.2997, acc: 0.00%] [G loss: 0.3276]\n",
            "Epoch 7000/30000 [D loss: 1.2996, acc: 0.00%] [G loss: 0.3276]\n",
            "Epoch 7100/30000 [D loss: 1.2996, acc: 0.00%] [G loss: 0.3276]\n",
            "Epoch 7200/30000 [D loss: 1.2996, acc: 0.00%] [G loss: 0.3275]\n",
            "Epoch 7300/30000 [D loss: 1.2996, acc: 0.00%] [G loss: 0.3275]\n",
            "Epoch 7400/30000 [D loss: 1.2995, acc: 0.00%] [G loss: 0.3275]\n",
            "Epoch 7500/30000 [D loss: 1.2995, acc: 0.00%] [G loss: 0.3274]\n",
            "Epoch 7600/30000 [D loss: 1.2995, acc: 0.00%] [G loss: 0.3274]\n",
            "Epoch 7700/30000 [D loss: 1.2994, acc: 0.00%] [G loss: 0.3274]\n",
            "Epoch 7800/30000 [D loss: 1.2994, acc: 0.00%] [G loss: 0.3274]\n",
            "Epoch 7900/30000 [D loss: 1.2993, acc: 0.00%] [G loss: 0.3273]\n",
            "Epoch 8000/30000 [D loss: 1.2993, acc: 0.00%] [G loss: 0.3273]\n",
            "Epoch 8100/30000 [D loss: 1.2992, acc: 0.00%] [G loss: 0.3273]\n",
            "Epoch 8200/30000 [D loss: 1.2992, acc: 0.00%] [G loss: 0.3273]\n",
            "Epoch 8300/30000 [D loss: 1.2992, acc: 0.00%] [G loss: 0.3272]\n",
            "Epoch 8400/30000 [D loss: 1.2992, acc: 0.00%] [G loss: 0.3272]\n",
            "Epoch 8500/30000 [D loss: 1.2991, acc: 0.00%] [G loss: 0.3272]\n",
            "Epoch 8600/30000 [D loss: 1.2991, acc: 0.00%] [G loss: 0.3272]\n",
            "Epoch 8700/30000 [D loss: 1.2991, acc: 0.00%] [G loss: 0.3271]\n",
            "Epoch 8800/30000 [D loss: 1.2991, acc: 0.00%] [G loss: 0.3271]\n",
            "Epoch 8900/30000 [D loss: 1.2990, acc: 0.00%] [G loss: 0.3271]\n",
            "Epoch 9000/30000 [D loss: 1.2990, acc: 0.00%] [G loss: 0.3271]\n",
            "Epoch 9100/30000 [D loss: 1.2990, acc: 0.00%] [G loss: 0.3271]\n",
            "Epoch 9200/30000 [D loss: 1.2989, acc: 0.00%] [G loss: 0.3270]\n",
            "Epoch 9300/30000 [D loss: 1.2989, acc: 0.00%] [G loss: 0.3270]\n",
            "Epoch 9400/30000 [D loss: 1.2989, acc: 0.00%] [G loss: 0.3270]\n",
            "Epoch 9500/30000 [D loss: 1.2989, acc: 0.00%] [G loss: 0.3270]\n",
            "Epoch 9600/30000 [D loss: 1.2989, acc: 0.00%] [G loss: 0.3270]\n",
            "Epoch 9700/30000 [D loss: 1.2989, acc: 0.00%] [G loss: 0.3269]\n",
            "Epoch 9800/30000 [D loss: 1.2988, acc: 0.00%] [G loss: 0.3269]\n",
            "Epoch 9900/30000 [D loss: 1.2988, acc: 0.00%] [G loss: 0.3269]\n",
            "Epoch 10000/30000 [D loss: 1.2988, acc: 0.00%] [G loss: 0.3269]\n",
            "Epoch 10100/30000 [D loss: 1.2988, acc: 0.00%] [G loss: 0.3269]\n",
            "Epoch 10200/30000 [D loss: 1.2988, acc: 0.00%] [G loss: 0.3269]\n",
            "Epoch 10300/30000 [D loss: 1.2988, acc: 0.00%] [G loss: 0.3268]\n",
            "Epoch 10400/30000 [D loss: 1.2988, acc: 0.00%] [G loss: 0.3268]\n",
            "Epoch 10500/30000 [D loss: 1.2987, acc: 0.00%] [G loss: 0.3268]\n",
            "Epoch 10600/30000 [D loss: 1.2987, acc: 0.00%] [G loss: 0.3268]\n",
            "Epoch 10700/30000 [D loss: 1.2987, acc: 0.00%] [G loss: 0.3268]\n",
            "Epoch 10800/30000 [D loss: 1.2987, acc: 0.00%] [G loss: 0.3268]\n",
            "Epoch 10900/30000 [D loss: 1.2987, acc: 0.00%] [G loss: 0.3267]\n",
            "Epoch 11000/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3267]\n",
            "Epoch 11100/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3267]\n",
            "Epoch 11200/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3267]\n",
            "Epoch 11300/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3267]\n",
            "Epoch 11400/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3267]\n",
            "Epoch 11500/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3267]\n",
            "Epoch 11600/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3267]\n",
            "Epoch 11700/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3266]\n",
            "Epoch 11800/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3266]\n",
            "Epoch 11900/30000 [D loss: 1.2986, acc: 0.00%] [G loss: 0.3266]\n",
            "Epoch 12000/30000 [D loss: 1.2985, acc: 0.00%] [G loss: 0.3266]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}